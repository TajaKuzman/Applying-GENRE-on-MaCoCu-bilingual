{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The genre prediction pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions of functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download_unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_unzip(url, folder_name):\n",
    "\t\"\"\"\n",
    "\tThe function downloads the zipped folder from an URL (from the CLARIN.SI repository),\n",
    "\tunzips it and saves the TMX file. It should be used for the MaCoCu data which is zipped\n",
    "\tusing the GZ appendix and the name of the file is the same as the name of the folder. It should be followed by the tmx_to_json function.\n",
    "\n",
    "\tArgs:\n",
    "\t- url(string): the URL from which the file can be downloaded\n",
    "\t- folder_name (string): name of the zipped folder; without \".GZ\"\n",
    "\t\"\"\"\n",
    "\timport gzip\n",
    "\timport shutil\n",
    "\timport wget\n",
    "\n",
    "\t# Downloading the file by sending the request to the URL\n",
    "\tcorpus_file = wget.download(url)\n",
    "\tprint('Downloading Completed')\n",
    "\n",
    "\t# Unzip the file\n",
    "\twith gzip.open(f'{folder_name}.gz', 'rb') as f_in:\n",
    "\t\twith open(f'{folder_name}', 'wb') as f_out:\n",
    "\t\t\tshutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tmx_to_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tmx_to_json(file_name, lang_code):\n",
    "\t\"\"\"\n",
    "\tTakes the TMX file of the MaCoCu corpora and transforms it into a JSON.\n",
    "\tIt saves the JSON file to which preprocess function is to be applied.\n",
    "\n",
    "\tArgs:\n",
    "\t- file_name (string): name of the TMX file\n",
    "\t- lang_code (string): the language code for the language, used along English - it is the same as in the name of the corpus (e.g. \"mk\" for MaCoCu-mk-en)\n",
    "\t\"\"\"\n",
    "\timport regex as re\n",
    "\timport json\n",
    "\t\n",
    "\tcorpus = open(f\"{file_name}\", \"r\").read()\n",
    "\tcorpus_sample = open(f\"{file_name}\", \"r\").read(5000)\n",
    "\n",
    "\t# Prepare all the regexes\n",
    "\t# Compile all tus\n",
    "\ttu_re = re.compile('<tu tuid=\".*?>\\n(.*?)<\\/tu>', re.DOTALL)\n",
    "\n",
    "\t# Compile relevant information inside tus\n",
    "\tbi_score_re = re.compile('<prop type=\"score-bicleaner-ai\">(.*?)</prop>')\n",
    "\tbiroamer_re = re.compile('<prop type=\"biroamer-entities\">(.*?)</prop>')\n",
    "\ttranslation_dir_re = re.compile('<prop type=\"translation-direction\">(.*?)</prop>')\n",
    "\ten_source_re = re.compile('<tuv xml:lang=\"en\">.*?<prop type=\"source-document\">(.*?)</prop>', re.DOTALL)\n",
    "\ten_par_id_re = re.compile('<tuv xml:lang=\"en\">.*?<prop type=\"paragraph-id\">(.*?)</prop', re.DOTALL)\n",
    "\ten_par_re = re.compile('<tuv xml:lang=\"en\">.*?<seg>(.*?)</seg>', re.DOTALL)\n",
    "\ten_var_doc_re = re.compile('<prop type=\"english-variant-document\">(.*?)</prop>')\n",
    "\ten_var_dom_re = re.compile('<prop type=\"english-variant-domain\">(.*?)</prop>')\n",
    "\tsl_source_re = re.compile(f'<tuv xml:lang=\"{lang_code}\">.*?<prop type=\"source-document\">(.*?)</prop>', re.DOTALL)\n",
    "\tsl_par_id_re = re.compile(f'<tuv xml:lang=\"{lang_code}\">.*?<prop type=\"paragraph-id\">(.*?)</prop', re.DOTALL)\n",
    "\tsl_par_re = re.compile(f'<tuv xml:lang=\"{lang_code}\">.*?<seg>(.*?)</seg>', re.DOTALL)\n",
    "\n",
    "\t# Create a list of all tus from the sample corpus\n",
    "\ttus_list_sample = tu_re.findall(corpus_sample)\n",
    "\n",
    "\t# View the tus_list\n",
    "\tprint(\"A sample of the tus in the corpora:\\n\")\n",
    "\tprint(tus_list_sample[1])\n",
    "\n",
    "\t# Check if regexes work\n",
    "\tregexes =  [bi_score_re, biroamer_re, translation_dir_re, en_source_re, en_par_id_re, en_par_re, en_var_doc_re, en_var_dom_re, sl_source_re, sl_par_id_re, sl_par_re]\n",
    "\n",
    "\tprint(\"A check if regexes work:\")\n",
    "\n",
    "\tfor rex in regexes:\n",
    "\t\ttest_list = rex.findall(tus_list_sample[1])\n",
    "\t\tprint(test_list)\n",
    "\n",
    "\t# Create a list of all tus from the corpus\n",
    "\ttus_list = tu_re.findall(corpus)\n",
    "\tprint(\"\\n\\nAll tus from the corpora were extracted. The number of sentence pairs (tus) is:\")\n",
    "\tprint(len(tus_list))\n",
    "\n",
    "\t\t# Create a list of dictionaries from the tus_list based on regexes\n",
    "\ttus_content = []\n",
    "\n",
    "\tfor i in tus_list:\n",
    "\t\t# Find all relevant information based on regexes\n",
    "\t\tbi_score = bi_score_re.search(i).group(1)\n",
    "\t\tbiroamer = biroamer_re.search(i).group(1)\n",
    "\t\ttranslation_dir = translation_dir_re.search(i).group(1)\n",
    "\t\ten_source = en_source_re.search(i).group(1)\n",
    "\t\ten_par_id = en_par_id_re.search(i).group(1)\n",
    "\t\ten_par = en_par_re.search(i).group(1)\n",
    "\t\ten_var_doc = en_var_doc_re.search(i).group(1)\n",
    "\t\ten_var_dom = en_var_dom_re.search(i).group(1)\n",
    "\t\tsl_source = sl_source_re.search(i).group(1)\n",
    "\t\tsl_par_id = sl_par_id_re.search(i).group(1)\n",
    "\t\tsl_par = sl_par_re.search(i).group(1)\n",
    "\n",
    "\t\t# Add information to the dictionary\n",
    "\t\tcurrent_tu = {\"score_bicleaner_ai\": float(bi_score), \"biroamer_entities\": biroamer, \"translation_direction\": translation_dir, \"en_source\": en_source, \"en_par_id\": en_par_id, \"en_par\": en_par, \"en_var_doc\": en_var_doc, \"en_var_dom\": en_var_dom, f\"{lang_code}_source\": sl_source, f\"{lang_code}_par_id\": sl_par_id, f\"{lang_code}_par\": sl_par}\n",
    "\t\t# Append the dictionary to the list\n",
    "\t\ttus_content.append(current_tu)\n",
    "\n",
    "\tprint(\"\\n\\nThe JSON format created. A sample: \\n\")\n",
    "\t# Print some instances of the tus_content\n",
    "\tprint(tus_content[:2])\n",
    "\n",
    "\t# Save json\n",
    "\n",
    "\twith open(f\"MaCoCu-{lang_code}-en.json\", \"w\") as file:\n",
    "\t\tjson.dump(tus_content,file, indent= \"\")\n",
    "\t\n",
    "\tprint(f\"\\n\\nThe JSON file is saved as MaCoCu-{lang_code}-en.json.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(lang_code):\n",
    "\t\"\"\"\n",
    "\tTakes the JSON file name, created in the tmx_to_json function,\n",
    "\ttransforms it into a pandas DataFrame, preprocesses it\n",
    "\tand saves the final document-level CSV file to which filter_non_textual function is to be applied.\n",
    "\n",
    "\tArgs:\n",
    "\t- file name (str): the path to the JSON file\n",
    "\t- lang code: the code of the language that is in the pair with English,\tit is the same as in the name of the MaCoCu file (e.g., mk in MaCoCu-mk-en)\n",
    "\t\"\"\"\n",
    "\timport pandas as pd\n",
    "\timport numpy as np\n",
    "\timport regex as re\n",
    "\timport json\n",
    "\n",
    "\twith open(f\"MaCoCu-{lang_code}-en.json\", \"r\") as file:\n",
    "\t\ttus_content = json.load(file)\n",
    "\n",
    "\t# Convert data to a dataframe\n",
    "\tcorpus_df = pd.DataFrame(tus_content)\n",
    "\n",
    "\t# Sort by english url and then by en_par_id to order the paragraphs into texts\n",
    "\tcorpus_df = corpus_df.sort_values(by = [\"en_source\", \"en_par_id\"])\n",
    "\n",
    "\t# Add information about domains\n",
    "\tdomain_re=re.compile(r'^https?://(?:www\\.)?(.+?)[/$]')\n",
    "\n",
    "\ten_domain_list = [domain_re.search(i).group(1) for i in corpus_df.en_source.to_list()]\n",
    "\n",
    "\tcorpus_df[\"en_domain\"] = en_domain_list\n",
    "\n",
    "\t# Repeat with domain of the other language\n",
    "\tsl_domain_list = [domain_re.search(i).group(1) for i in corpus_df[f\"{lang_code}_source\"].to_list()]\n",
    "\tcorpus_df[f\"{lang_code}_domain\"] = sl_domain_list\n",
    "\n",
    "\t# Add information whether the domains are the same\n",
    "\tcorpus_df[\"same_domains\"] = np.where(corpus_df[\"en_domain\"] == corpus_df[f\"{lang_code}_domain\"], \"yes\", 'no')\n",
    "\n",
    "\t# Add column for domains that are different\n",
    "\tcorpus_df[\"different_domains\"] = corpus_df[\"en_domain\"] + \" \" + corpus_df[f\"{lang_code}_domain\"]\n",
    "\n",
    "\t# Print the information\n",
    "\tprint(\"Information about the web domains for the two languages is added. See the head of the dataframe:\\n\")\n",
    "\tdisplay(corpus_df.head(2))\n",
    "\n",
    "\tprint(\"Number of same and different domains in the corpus:\\n\")\n",
    "\n",
    "\tprint(corpus_df[\"same_domains\"].value_counts().to_markdown())\n",
    "\n",
    "\t# Number of texts and sentences up to now\n",
    "\tprevious_no_sentences = corpus_df.en_source.count()\n",
    "\tprevious_no_texts = len(corpus_df.en_source.unique())\n",
    "\tprint(f\"\\nCurrent number of sentences: {previous_no_sentences}\")\n",
    "\tprint(f\"Current number of texts: {previous_no_texts}\\n\\n\")\n",
    "\n",
    "\t# See number of discarded texts and sentences\n",
    "\tdef calculate_discarded(previous_no_sentences, previous_no_texts, calculate_texts_only):\n",
    "\t\tnew_number_sentences = corpus_df.en_source.count()\n",
    "\t\tnew_number_texts = len(corpus_df.en_source.unique())\n",
    "\t\tif calculate_texts_only == False:\n",
    "\t\t\tprint(f\"New number of sentences: {new_number_sentences}\")\n",
    "\t\t\tprint(f\"No. of discarded sentences: {previous_no_sentences-new_number_sentences}, percentage: {(previous_no_sentences-new_number_sentences)/previous_no_sentences}\")\n",
    "\t\t\n",
    "\t\tprint(f\"New number of texts: {new_number_texts}\")\n",
    "\t\tprint(f\"No. of discarded texts: {previous_no_texts-new_number_texts}, percentage: {(previous_no_texts-new_number_texts)/previous_no_texts}\")\n",
    "\n",
    "\t\treturn new_number_sentences, new_number_texts\n",
    "\t\n",
    "\t# Discard instances that are from different domains\n",
    "\tcorpus_df = corpus_df[corpus_df[\"same_domains\"] == \"yes\"]\n",
    "\n",
    "\tprint(\"Instances from different domains were discarded.\\n\")\n",
    "\n",
    "\tsentences_same_domains, texts_same_domains = calculate_discarded(previous_no_sentences, previous_no_texts, False)\n",
    "\n",
    "\t# Calculate average bicleaner ai score based on the en_source\n",
    "\tcorpus_df[\"average_score\"] = corpus_df[\"score_bicleaner_ai\"].groupby(corpus_df['en_source']).transform('mean')\n",
    "\n",
    "\t# Join par id and text\n",
    "\tcorpus_df[\"en-par-text\"] = corpus_df[\"en_par_id\"] + \"-\" + corpus_df[\"en_par\"]\n",
    "\n",
    "\t# Discard all duplicated English paragraphs with the same par id\n",
    "\tcorpus_df = corpus_df.drop_duplicates(\"en-par-text\")\n",
    "\n",
    "\tprint(\"\\nAll duplicated English sentences with the same paragraph and sentence ID were discarded.\\n\")\n",
    "\n",
    "\tsentences_dupl_sent, text_dupl_sent = calculate_discarded(sentences_same_domains, texts_same_domains, False)\n",
    "\n",
    "\t# Add to each instance from the same en_source joint text from all sentences\n",
    "\tcorpus_df[\"en_doc\"] = corpus_df[\"en_par\"].groupby(corpus_df['en_source']).transform(' '.join)\n",
    "\n",
    "\t# Repeat with the text in other language\n",
    "\tcorpus_df[f\"{lang_code}_doc\"] = corpus_df[f\"{lang_code}_par\"].groupby(corpus_df[f'{lang_code}_source']).transform(' '.join)\n",
    "\n",
    "\t# Keep only one example of each text\n",
    "\tcorpus_df = corpus_df.drop_duplicates(\"en_doc\")\n",
    "\n",
    "\tprint(\"\\nThe sentences were merged into texts based on the source URL and the English duplicated texts were removed.\\n\")\n",
    "\n",
    "\tsentences_after_text_deduplication, texts_after_text_deduplication = calculate_discarded(sentences_dupl_sent, text_dupl_sent, True)\n",
    "\n",
    "\t# Add information about length\n",
    "\tcorpus_df[\"en_length\"] = corpus_df.en_doc.str.split().str.len()\n",
    "\n",
    "\t# Add information about length of the other language\n",
    "\tcorpus_df[f\"{lang_code}_length\"] = corpus_df[f\"{lang_code}_doc\"].str.split().str.len()\n",
    "\n",
    "\n",
    "\tprint(\"\\nInitial length of texts in the corpus:\")\n",
    "\n",
    "\tprint(corpus_df.en_length.describe().to_markdown())\n",
    "\n",
    "\t# Discard instances that have length less than  79 (median from other datasets)\n",
    "\tcorpus_df = corpus_df[corpus_df[\"en_length\"] > 78]\n",
    "\n",
    "\tprint(\"\\nTexts that have less than 79 words were discarded.\\n\")\n",
    "\n",
    "\tsentences_after_length, texts_after_length = calculate_discarded(sentences_after_text_deduplication, texts_after_text_deduplication, True)\n",
    "\n",
    "\t# Discard irrelevant columns\n",
    "\tcorpus_df = corpus_df.drop(columns = ['score_bicleaner_ai', 'en_par_id', 'en_par', f'{lang_code}_par_id', f'{lang_code}_par', 'en-par-text', 'same_domains', 'different_domains'])\n",
    "\n",
    "\t# View the final dataframe\n",
    "\tprint(\"The final dataframe: \\n\")\n",
    "\n",
    "\tdisplay(corpus_df.head(5))\n",
    "\n",
    "\t# Save the dataframe to csv\n",
    "\tcorpus_df.to_csv(f\"Macocu-{lang_code}-en-doc-format.csv\", sep= \"\\t\")\n",
    "\n",
    "\tprint(f\"The preparation of the file is finished and the file is saved as Macocu-{lang_code}-en-doc-format.csv.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter_non_textual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_non_textual(lang_code, lower_limit = 0.015, upper_limit = 0.2):\n",
    "\t\"\"\"\n",
    "\tTakes the CSV file, produced with the preprocess function\n",
    "\tand applies filtering of the non-textual texts based on a no. of punctuations per no. of words heuristic.\n",
    "\n",
    "\tArgs:\n",
    "\t- file_name (str): path to the CSV file, without the \".csv\" (!)\n",
    "\t- lower_limit (float): default is 0.015, can be changed if the results show that this would filter out mostly okay texts\n",
    "\t- upper_limit (float): default is 0.2, can be changed if the results show that this would filter out mostly okay texts\n",
    "\t\n",
    "\tSaves the filtered dataframe as a CSV to which genre predictions are to be made.\n",
    "\t\"\"\"\n",
    "\timport pandas as pd\n",
    "\timport regex as re\n",
    "\n",
    "\tcorpus_df = pd.read_csv(f\"Macocu-{lang_code}-en-doc-format.csv\", sep= \"\\t\", index_col = 0)\n",
    "\n",
    "\t# Filter out the non-textual texts\n",
    "\n",
    "\t# Calculate ratio of punctuations per words\n",
    "\n",
    "\tdef paragraph_punct_ratio(text):\n",
    "\t\ttoken_re=re.compile(r'\\w+|\\S',re.UNICODE)\n",
    "\t\ttokens=token_re.findall(text)\n",
    "\t\tpunct=len([e for e in tokens if e in '.;,!?:'])\n",
    "\t\tratio = punct/len(tokens)\n",
    "\t\treturn ratio\n",
    "\n",
    "\tcorpus_df[\"punct_ratio\"] = corpus_df.en_doc.apply(paragraph_punct_ratio)\n",
    "\n",
    "\tprint(f\"Texts (first 5) that would be discarded with the lower limit: {lower_limit}\\n\")\n",
    "\n",
    "\t# With the ratio below the lower limit, we catch non-textual texts without any punctuation\n",
    "\tfor i in corpus_df.query(f\"punct_ratio < {lower_limit}\").en_doc.to_list()[:5]:\n",
    "\t\tprint(i)\n",
    "\n",
    "\tprint(f\"\\n\\nTexts (first 5) that would be discarded with the upper limit: {upper_limit}\\n\")\n",
    "\n",
    "\t# With ratio above the upper limit, we catch non-textual texts with a lot of punctuations\n",
    "\tfor i in corpus_df.query(f\"punct_ratio > {upper_limit}\").en_doc.to_list()[:5]:\n",
    "\t\tprint(i)\n",
    "\n",
    "\t# Number of texts up to now\n",
    "\tprevious_no_texts = len(corpus_df.en_source.unique())\n",
    "\n",
    "\t# See number of discarded texts and sentences\n",
    "\tdef calculate_discarded(previous_no_sentences, previous_no_texts, calculate_texts_only):\n",
    "\t\tnew_number_sentences = corpus_df.en_source.count()\n",
    "\t\tnew_number_texts = len(corpus_df.en_source.unique())\n",
    "\t\tif calculate_texts_only == False:\n",
    "\t\t\tprint(f\"New number of sentences: {new_number_sentences}\")\n",
    "\t\t\tprint(f\"No. of discarded sentences: {previous_no_sentences-new_number_sentences}, percentage: {(previous_no_sentences-new_number_sentences)/previous_no_sentences}\")\n",
    "\t\t\n",
    "\t\tprint(f\"New number of texts: {new_number_texts}\")\n",
    "\t\tprint(f\"No. of discarded texts: {previous_no_texts-new_number_texts}, percentage: {(previous_no_texts-new_number_texts)/previous_no_texts}\")\n",
    "\n",
    "\t\treturn new_number_sentences, new_number_texts\n",
    "\n",
    "\t# Filter the corpus by using only instances with ratio between the lower and upper limit\n",
    "\tcorpus_df = corpus_df.query(f\"punct_ratio >= {lower_limit} & punct_ratio <= {upper_limit}\")\n",
    "\n",
    "\tprint(\"The non-textual texts were discarded.\\n\")\n",
    "\n",
    "\tsentences_after_heuristic, texts_after_heuristic = calculate_discarded(100, previous_no_texts, True)\n",
    "\n",
    "\tdisplay(corpus_df.head(5))\n",
    "\n",
    "\t# Save the dataframe to csv\n",
    "\tcorpus_df.to_csv(f\"Macocu-{lang_code}-en-doc-format-filtered.csv\", sep= \"\\t\")\n",
    "\n",
    "\tprint(f\"The preparation of the file is finished and the file is saved as Macocu-{lang_code}-en-doc-format-filtered.csv.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analyze_prepared_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_prepared_corpus(lang_code):\n",
    "\t\"\"\"\n",
    "\tTakes the CSV file, created by the filter_non_textual function and analyzes the corpus.\n",
    "\n",
    "\tArgs:\n",
    "\t- file_name (str): path to the CSV file\n",
    "\t\"\"\"\n",
    "\timport pandas as pd\n",
    "\n",
    "\tcorpus_df = pd.read_csv(f\"Macocu-{lang_code}-en-doc-format-filtered.csv\", sep= \"\\t\", index_col = 0)\n",
    "\n",
    "\tprint(\"View the corpus:\")\n",
    "\tdisplay(corpus_df.head(3))\n",
    "\n",
    "\t# Inspect corpus information\n",
    "\tprint(\"All information about the corpus: \\n\")\n",
    "\tdisplay(corpus_df.describe(include=\"all\"))\n",
    "\n",
    "\t# Inspect en_var_doc statistics\n",
    "\n",
    "\tprint(\"\\nPrediction of English varieties (on document level):\\n\")\n",
    "\tprint(corpus_df.en_var_doc.value_counts(normalize = True).to_markdown())\n",
    "\n",
    "\tprint(\"\\nPrediction of English varieties (on domain level):\\n\")\n",
    "\tprint(corpus_df.en_var_dom.value_counts(normalize = True).to_markdown())\n",
    "\n",
    "\t# Inspect translation direction\n",
    "\tprint(\"\\nPrediction of translation direction:\\n\")\n",
    "\tprint(corpus_df.translation_direction.value_counts(normalize = True).to_markdown())\n",
    "\n",
    "\tprint(\"\\nInformation on the bicleaner score:\\n\")\n",
    "\tprint(corpus_df.average_score.describe().to_markdown())\n",
    "\n",
    "\tprint(\"\\nFinal length of texts in the corpus:\")\n",
    "\tprint(corpus_df.en_length.describe().to_markdown())\n",
    "\t\n",
    "\t# Analyze English domains in the corpus_df\n",
    "\tcount = pd.DataFrame({\"Count\": list(corpus_df.en_domain.value_counts())[:30], \"Percentage\": list(corpus_df.en_domain.value_counts(normalize=\"True\")*100)[:30]}, index = corpus_df.en_domain.value_counts()[:30].index)\n",
    "\n",
    "\tprint(\"\\nAn analysis of the 30 most frequent English domains:\")\n",
    "\tprint(count.to_markdown())\n",
    "\n",
    "\tprint(\"\\n\\nAnalysis completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "postprocess_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_results(file_name, lang_code):\n",
    "\t\"\"\"\n",
    "\tTakes the CSV file with genre predictions, applies filtering - discards some of the non-reliable results,\n",
    "\tand saves the final file as CSV.\n",
    "\n",
    "\tArgs:\n",
    "\t- file_name: path to the CSV file with predictions\n",
    "\t\"\"\"\n",
    "\timport pandas as pd\n",
    "\timport numpy as np\n",
    "\n",
    "\tcorpus = pd.read_csv(f\"{file_name}\", sep = \"\\t\", index_col = 0)\n",
    "\n",
    "\t# View the Dataframe\n",
    "\tdisplay(corpus.head(3))\n",
    "\n",
    "\t# Analyze genre distribution\n",
    "\tcount = pd.DataFrame({\"Count\": list(corpus[\"X-GENRE\"].value_counts()), \"Percentage\": list(corpus[\"X-GENRE\"].value_counts(normalize=\"True\")*100)}, index = corpus[\"X-GENRE\"].value_counts().index)\n",
    "\n",
    "\tprint(\"Genre distribution before post-processing: \\n\")\n",
    "\tprint(count.to_markdown())\n",
    "\n",
    "\tinitial_number_of_labels = corpus[\"X-GENRE\"].count()\n",
    "\n",
    "\t# Post-process the data\n",
    "\n",
    "\t# Copy all predicted labels to a new column, except if the label is \"Other\"\n",
    "\tcorpus[\"final-X-GENRE\"] = np.where(corpus[\"X-GENRE\"] == \"Other\", np.nan, corpus[\"X-GENRE\"])\n",
    "\n",
    "\t# Copy all predicted labels to a column \"final-X-GENRE\", except if the label is \"Forum\"\n",
    "\tcorpus[\"final-X-GENRE\"] = np.where(corpus[\"final-X-GENRE\"] == \"Forum\", np.nan, corpus[\"final-X-GENRE\"])\n",
    "\n",
    "\tprint(\"The Forum and Other label were discarded from the column with final genre labels.\")\n",
    "\n",
    "\tprint(\"New genre distribution:\\n\")\n",
    "\tprint(corpus[\"final-X-GENRE\"].value_counts().to_markdown())\n",
    "\n",
    "\tcurrent_no_final_labels = corpus[\"final-X-GENRE\"].count()\n",
    "\n",
    "\t# Copy all predicted labels to a column \"final-X-GENRE\", except if the prediction confidence is lower than 0.9\n",
    "\tcorpus[\"final-X-GENRE\"] = np.where(corpus[\"chosen_category_distr\"] < 0.9, np.nan, corpus[\"final-X-GENRE\"])\n",
    "\n",
    "\tprint(\"Labels, predicted with confidence, lower than 0.9, were discarded from the final labels.\\n\")\n",
    "\n",
    "\tfinal_no_of_labels = corpus[\"final-X-GENRE\"].count()\n",
    "\n",
    "\tprint(f\"Number of discarded labels due to confidence being to low: {current_no_final_labels-final_no_of_labels}, percentage: {(current_no_final_labels-final_no_of_labels)/current_no_final_labels}\")\n",
    "\n",
    "\tprint(f\"Final number of labelled texts: {final_no_of_labels}\")\n",
    "\n",
    "\tprint(f\"Total number of labels discarded due to post-processing: {initial_number_of_labels-final_no_of_labels}, percentage: {(initial_number_of_labels-final_no_of_labels)/initial_number_of_labels}\")\n",
    "\n",
    "\t# Analyze final genre distribution\n",
    "\tcount = pd.DataFrame({\"Count\": list(corpus[\"final-X-GENRE\"].value_counts()), \"Percentage\": list(corpus[\"final-X-GENRE\"].value_counts(normalize=\"True\")*100)}, index = corpus[\"final-X-GENRE\"].value_counts().index)\n",
    "\n",
    "\tprint(\"Final genre distribution:\\n\")\n",
    "\tprint(count.to_markdown())\n",
    "\n",
    "\tLABELS = list(corpus[\"final-X-GENRE\"].unique())\n",
    "\n",
    "\t# Save the new file\n",
    "\tcorpus.to_csv(f\"Macocu-{lang_code}-en-predicted-post-processed.csv\")\n",
    "\n",
    "\tprint(f\"The file with final labels is saved as {file_name}-post-processed.csv.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analyze_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(lang_code, no_of_domains):\n",
    "\t\"\"\"\n",
    "\tTakes the post-processed CSV file with genre predictions, produced with the postprocess_results function and analyzes the results.\n",
    "\tSaves a file with results, named \"file_name-analysis.txt\"\n",
    "\n",
    "\tArgs:\n",
    "\t- file_name (str): path to the post-processed CSV file with predictions\n",
    "\t- no_of_domains (int): define how many most frequent domains you want to analyze in terms of genre distribution - usually, we take the number of domains that represent more than 1% of data (this is analyzed in analyze_prepared_corpus)\n",
    "\t\"\"\"\n",
    "\timport pandas as pd\n",
    "\timport numpy as np\n",
    "\n",
    "\tcorpus = pd.read_csv(f\"Macocu-{lang_code}-en-predicted-post-processed.csv\", index_col = 0)\n",
    "\n",
    "\tresults_file = open(f\"MaCoCu-{lang_code}-en-predicted-analysis.txt\", \"w\")\n",
    "\tresults_file.write(f\"Analysis of results for file: Macocu-{lang_code}-en-predicted-post-processed.csv\\n\\n\")\n",
    "\n",
    "\t# View the Dataframe\n",
    "\tprint(\"View the final dataframe:\\n\\n\")\n",
    "\tdisplay(corpus.head(3))\n",
    "\n",
    "\t# Analyze English domains in the corpus\n",
    "\tcount = pd.DataFrame({\"Count\": list(corpus.en_domain.value_counts()), \"Percentage\": list(corpus.en_domain.value_counts(normalize=\"True\")*100)}, index = corpus.en_domain.value_counts().index)\n",
    "\n",
    "\tdomains_to_analyse = count.index.to_list()[:no_of_domains]\n",
    "\n",
    "\t# See the distribution of genres in the most frequent domains:\n",
    "\tresults_file.write(\"Distribution of genres in the most frequent domains:\\n\\n\")\n",
    "\n",
    "\tfor i in domains_to_analyse:\n",
    "\t\tresults_file.write(\"\\n\\n\")\n",
    "\t\tresults_file.write(i)\n",
    "\t\tresults_file.write(\"\\n\\n\")\n",
    "\t\tfiltered_corpus = corpus[corpus[\"en_domain\"] == i]\n",
    "\t\tresults_file.write(filtered_corpus[\"final-X-GENRE\"].value_counts(normalize=\"True\").to_markdown())\n",
    "\n",
    "\tprint(\"The distribution of genres in the most frequent domains analyzed.\")\n",
    "\n",
    "\t# Analyze differences in genres based on domain frequency\n",
    "\tresults_file.write(\"\\n\\nDistribution of domains in genres:\\n\\n\")\n",
    "\n",
    "\tfor i in ['Opinion/Argumentation', 'News', 'Legal', 'Information/Explanation', 'Promotion', 'Instruction', 'Prose/Lyrical']:\n",
    "\t\tresults_file.write(\"\\n\\n\")\n",
    "\t\tresults_file.write(i)\n",
    "\t\tresults_file.write(\"\\n\\n\")\n",
    "\t\tfiltered_corpus = corpus[corpus[\"final-X-GENRE\"] == i]\n",
    "\t\tresults_file.write(filtered_corpus[\"en_domain\"].value_counts(normalize=\"True\")[:5].to_markdown())\n",
    "\n",
    "\tprint(\"Differences of the domain distribution for each genre analyzed.\")\n",
    "\n",
    "\t# Analyze differences in genres based on language varieties\n",
    "\tresults_file.write(\"\\n\\nDistribution of English varieties in genres:\\n\\n\")\n",
    "\n",
    "\tfor i in ['News', 'Opinion/Argumentation', 'Promotion', 'Instruction', 'Information/Explanation', 'Legal', 'Prose/Lyrical']:\n",
    "\t\tresults_file.write(\"\\n\\n\")\n",
    "\t\tresults_file.write(i)\n",
    "\t\tresults_file.write(\"\\n\\n\")\n",
    "\t\tfiltered_corpus = corpus[corpus[\"final-X-GENRE\"] == i]\n",
    "\t\tresults_file.write(filtered_corpus[\"en_var_doc\"].value_counts(normalize=\"True\").to_markdown())\n",
    "\n",
    "\tprint(\"Differences in language varieties distribution per genres analyzed.\")\n",
    "\n",
    "\t# Length distribution of the entire corpus\n",
    "\tresults_file.write(\"\\n\\nLength distribution of the entire corpus:\\n\\n\")\n",
    "\tresults_file.write(corpus[\"en_length\"].describe().to_markdown())\n",
    "\n",
    "\t# Analyze differences in genres based on text length\n",
    "\tresults_file.write(\"\\n\\nLength distribution for each of the genre subcorpus:\\n\\n\")\n",
    "\n",
    "\tfor i in ['News', 'Opinion/Argumentation', 'Promotion', 'Instruction', 'Information/Explanation', 'Legal', 'Prose/Lyrical']:\n",
    "\t\tresults_file.write(\"\\n\\n\")\n",
    "\t\tresults_file.write(i)\n",
    "\t\tresults_file.write(\"\\n\\n\")\n",
    "\t\tfiltered_corpus = corpus[corpus[\"final-X-GENRE\"] == i]\n",
    "\t\tresults_file.write(filtered_corpus[\"en_length\"].describe().to_markdown())\n",
    "\t\n",
    "\tresults_file.close()\n",
    "\t\n",
    "\tprint(f\"Analysis completed. Inspect the result file MaCoCu-{lang_code}-en-predicted-analysis.txt for results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete pipeline of MaCoCu corpus preparation for genre prediction\n",
    "\n",
    "# Define the URL from which the MaCoCu corpus is to be downloaded:\n",
    "url = \n",
    "\n",
    "# Define the name of the zipped folder; without \".GZ\":\n",
    "folder_name =\n",
    "\n",
    "# Define the language code of the language that is in the combination with English\n",
    "# (it is in the name of the MaCoCu file, e.g. \"mk\" in Macocu-mk-en)\n",
    "lang_code = \n",
    "\n",
    "# After the prediction of genres is complete, run the post-processing and analysis of results\n",
    "# Define the name of the file with predictions (stated in the predict_genres.py)\n",
    "file_name = \"\"\n",
    "\n",
    "# Define the number of the most frequent domains you wish to analyze\n",
    "# (the best is to choose domains that are present in more than 1% of data - this information\n",
    "# is obtained with the analyze_prepared_corpus function.)\n",
    "no_of_domains = 7\n",
    "\n",
    "# No need to change anything below this point.\n",
    "# -------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and unzip the files\n",
    "download_unzip(url, folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the downloaded TMX file to JSON\n",
    "tmx_to_json(folder_name, lang_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the JSON file, transformed to a DataFrame and save it to the CSV file\n",
    "preprocess(lang_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply final preprocessing step: filter out non-textual texts if applicable (change lower and upper limit if necessary)\n",
    "filter_non_textual(lang_code, lower_limit = 0.015, upper_limit = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prepared corpus\n",
    "analyze_prepared_corpus(lang_code)\n",
    "\n",
    "# After preparation of the corpus, you can apply genre prediction to it - modify the beginning of the predict_genres.py file and run it in the terminal:\n",
    "# nohup python predict_genres.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-process the results\n",
    "postprocess_results(lang_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the results\n",
    "analyze_results(lang_code, no_of_domains)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e373e41fe05b496006fe2fc132d7af19f1d513370c44925a0044a5f3ee41336"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
